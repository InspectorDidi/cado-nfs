The cadofactor Python script is generally run with

./cadofactor.py parameterfile

All the parameters for the factorization are read from the parameter file,
but it is possible to specify such parameters on the command line, after
parameterfile.

For example, running

./cadofactor.py ../../params/params.c59 N=90377629292003121684002147101760858109247336549001090677693 tasks.workdir=/tmp/c59 tasks.execpath=$HOME/build/cado-nfs/normal server.whitelist=0.0.0.0/0

starts the cadofactor script, which also starts the server. It does not start any clients with this command line, so those would have to be started manually:

./wuclient2.py --server=https://quiche.loria.fr:8001 --certsha1=[Certificate SHA1]

where the --server and --certsha1 parameters should be given the URL and certificate SHA1 value of the server,
respectively, as printed by the cadofactor script. You can start an arbitrary number of client scripts, on any
machines that can connect to the server via HTTP.

If you want to let the server automatically start clients, you need to supply a list of hostnames on which to start clients, e.g.,

./cadofactor.py ../../params/params.c59 N=90377629292003121684002147101760858109247336549001090677693 tasks.workdir=/tmp/c59 tasks.execpath=$HOME/build/cado-nfs/normal slaves.hostnames=localhost slaves.nrclients=2 slaves.scriptpath=$HOME/git/cado-nfs/scripts/cadofactor

to let it start two clients on localhost. The scriptpath parameter must be the path to the directory on the client machine which contains wuclient2.py (and workunit.py).


For complex set-ups, it is preferable to write a parameter file. Some examples are in "parameters", "parameters.oar", and "parameters.rsa512.oar".
The params/params.c90 file contains extensive comments describing the most common parameters.

The .oar parameter files are meant for cadofactor scripts that run *inside* an OAR submission on clusters that use OAR as the job scheduler,
such as Grid5000, as they read the list of slave hostnames from the OAR node file.

For example,

oarsub -I
./cadofactor.py parameters.oar

factors the usual c59 test number using the nodes reserved via OAR, in this case one node.
The parameters.oar file contains the line
slaves.catrel.nrclients = 8
which tells the script to launch 8 clients on each unique host name (=node); the parameter
threads=2
causes all the programs to use two threads, resulting in 16 threads per node.

If a factorization is interrupted, it is usually possible to resume it where
it left off, simply by running cadofactor.py again with the same command
line parameters. These command line parameters are printed to the screen and
written to the .log file in the factorization's working directory on each
run.

If a factorization cannot be resumed for whatever reason, it is often
possible to salvage some of the output files which can then be imported into
a new factorization run of the same number, but with a new working
directory.


Importing a polynomial file
===========================

If you want to import a polynomial file (either an SNFS polynomial, or
if polynomial selection was already completed in an earlier run), use:

tasks.polyselect.import=xxx.poly

where xxx.poly is the name of the polynomial file (in CADO-NFS format).

This imports the polynomial and prevents any additional polynomial selection
from running, i.e., the imported polynomial is used unconditionally.


The polynomial selection run by cadofactor is performed in two phases:
the first phase searches for polynomials with good size property and keeps
in a priority queue the "nrkeep" most promising ones, then the second phase
root-optimizes these to find the overall best polynomial by Murphy E value.

You can import files that were previously produced by phase 1 (resp. phase 2)
into phase 1 (resp. phase 2) again; the imported files will be processed as if
phase 1 (resp. phase 2) had generated them by itself. Phase 1 will add any
polynomials from the imported files into its priority queue and forward the kept
ones to phase 2, while phase 2 will choose the best polynomial by Murphy E from
the imported ones and the ones it computes by itself.


If you want to import size-optimized polynomials into phase 1 of polynomial
selection and then continue polynomial search, use

tasks.polyselect.polyselect1.import_sopt=file
or
tasks.polyselect.polyselect1.import_sopt=@file

With @file, file should contain a list of files to import, one file name
per line. All polynomials from the given file(s) are imported and added to
the phase 1 priority queue, keeping the nrkeep best ones. Polynomial
selection phase 1 then continues normally. For example, if an earlier run
completed ad values up to ad=200000 and you want to resume the search from
there, you could use

tasks.polyselect.polyselect1.import_sopt=@list_of_existing_files tasks.polyselect.admin=200000

which imports the existing files, then resumes searching at ad=200000 up to
the admax value given in the parameter file.


If you want to import root-optimized polynomials into phase 2 of polynomial
selection and then continue polynomial search, use

tasks.polyselect.polyselect2.import_ropt=file
or
tasks.polyselect.polyselect2.import_ropt=@file

This reads the polynomial(s) in the given file(s), then root-optimizes any
polynomials found in phase 1; the best polynomial (rated by the Murphy E
value) will be used for the sieving.

Warning: if a polynomial file does not specify a Murphy E value and is
imported into phase 2, its Murphy E value is set to 0 by default. Since any
polynomials found by the polynomial search have positive Murphy E, the
imported one will always "lose". To import, e.g., an SNFS polynomial
without Murphy E, use "tasks.polyselect.import", and not "import_ropt".


Importing relations
===================

Important: when importing relations, it is assumed one also imports the
corresponding polynomial (a few relations from each file will be checked
against that polynomial). If no polynomial is imported, anything can happen.

If you want to import already computed relations, use:

tasks.sieve.import=foo

where "foo" is the name of the relation file to be imported (in CADO-NFS
format). Alternatively you can do:

tasks.sieve.import=@foo

where "foo" is a file containing several names of relation files, one file
name per line.

Adjust the tasks.sieve.qmin parameter accordingly to prevent
already-sieved special-q ranges from being sieved again. To do that:

* go into the cxx.upload directory
* list the cxx.sieving.xxx-yyy.*.gz files
* find the largest yyy value, and use it as new qmin

Be aware that importing relation files that were not produced by
CADO-NFS's siever (for instance, handcrafted relations) is not supported:
the script will assume various things, like the presence of certain
commented lines. The list of these particularities may vary over time,
so we don't even try to list them here.

File locking
============

The SqLite database used by cadofactor makes extensive use of file locking.
This requires that the underlying file system properly implements POSIX file
locking. One example of a file system that claims to, but does not, is
glusterfs, which leads to random SqLite errors. See the thread

http://gluster.org/pipermail/gluster-users/2011-September/031720.html

If you encounter "Database corrupted" or similar errors, try if storing the
working directory on a local (not network-mounted) file system resolves the
problem.

Controlling the filtering stage
===============================

By default CADO-NFS requires an excess (after the first singleton removal)
of 10% more relations than ideals. This is controlled by:

tasks.filter.purge.required_excess=0.1

If you want a larger excess, say 20%, just add on the cadofactor.py command
line (or in the parameter file):

tasks.filter.purge.required_excess=0.2

If any positive excess is enough:

tasks.filter.purge.required_excess=0.0

Note: if your factorization already started the linear algebra step, and you
want to do more sieving, you can restart it with a larger "rels_wanted" than
the current number of relations. For example if you have say 1000000 relations
(grep "is now" in the log file), just add in the cadofactor.py line:

If not enough relations are collected after the filtering step, the sieve is executed again with an additional number of wanted relations.
The parameter tasks.filter.purge.add_ratio controls the number of additional required relations as a ratio of the number of unique relations already collected:

tasks.filter.purge.add_ratio=0.05

specifies that you want 5% more relations before next filtering step.
The default value is 0.1 (i.e. 10%).

tasks.sieve.rels_wanted=1000001

Printing and manipulating the database
======================================

The wudb.py script can be used to print info on the workunits. For example,

./wudb.py -dbfile /tmp/work/testrun.db -dump -assigned

prints all currently assigned work units.

To cancel a workunit:

./wudb.py -dbfile /tmp/work/testrun.db -cancel -wuid wuname

where "wuname" appears after "Workunit" in the "-dump -assigned" call.

This script can also be used to manipulate the database, for example:

./wudb.py -dbfile /tmp/work/testrun.db -setdict sqrt next_dep int 0

sets the "next_dep" variable in the SqrtTask's state to the integer value 0.

Using several threads for the square root step
==============================================

By default the given number of threads (-t option of factor.sh, or
tasks.threads for cadofactor.py) is used in the square root step. To use say
16 threads in parallel, add the following to the cadofactor.py command line:

tasks.sqrt.threads = 16

