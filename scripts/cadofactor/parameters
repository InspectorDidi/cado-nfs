# Parameters form a tree structure. 
# A parameter of the form
# foo.bar.baz = 42
# is said to have path "foo.bar", key "baz", and value "42".

# All quotes in the comments in this file are for emphasis and not part of
# the parameters.

# Objects of the Python scripts that take parameters from the parameter file 
# have a path associated with them; for example, the Sieving task uses path
# "tasks.sieve", and the las siever run in the Sieving task uses
# "tasks.sieve.las". When looking for a parameter, we start at the object's 
# path, then work towards to root of the tree until the key is found.

# Some variable substitution occurs in the values specified in a parameter file:
# A string of the form "${FOO}" is substituted by the value of the shell
# environment variable "$FOO".
# A string of the form "$(foo)" is substituted by the value of the parameter
# with key "foo", where the search starts at the path where "$(foo)" occurred.


# name: The name of the factorization project. Will be used as part of file
# names, among other things, so probably should be alphanumeric.
# Everything needs to know the name of the factorization, so this should be
# specified at the root of the parameter tree.

name = test_run


# N: The number to be factored

N = 90377629292003121684002147101760858109247336549001090677693


# Adjust sourcedir to point to the root of your CADO source directory. 
# This parameter is not actually used by cadofactor directly, but it is 
# referenced as $(sourcedir) further down in the parameter file.

sourcedir=${HOME}/git/cado-nfs


# Adjust builddir to point to the root of your CADO build directory
# Not used directly, either, but referenced, too.
# This default value here points to the default build directory of
# CADO, but a problem is that HOSTNAME may not be an exported variable
# of the shell. In that case, the "${HOSTNAME}" here needs to be
# replaced, or the HOSTNAME variable needs to be exported manually.

builddir=$(sourcedir)/build/${HOSTNAME}


# Parameters for the various tasks performed during the factorization

# workdir: the path of the working directory; if a relative path is
# specified, it is relative to the CWD from which cadofactor is run.
# All the output files of sieving, filtering, etc., are stored under the
# working directory.
# In principle it is possible to specify a different working directory for
# each task, which may be beneficial to use disk space across several file
# systems, or to let filtering tasks read from one hard drive while writing
# to another, improving throughput.

tasks.workdir = /tmp/work/


# execpath: Specify the search path for binary programs that should be run.
# Each binary program that is run can be specified with "execpath",
# "execsubdir", and "execbin" parameters; the script looks for the binary
# executable as "execpath/execsubdir/execbin" and "execpath/execbin", in this
# order. Defaults for these parameters are defined for each program in
# cadoprograms.py.
# The Program instances use parameter path, e.g., "tasks.sieve.las", or
# "tasks.filter.merge.merge", but of course can find their parameter from 
# further up the parameter tree, such as in "tasks".

tasks.execpath=$(builddir)
# Example: if you wanted to use a debugging-enabled binary for las which
# is in "${HOME}/build/cado-nfs/debug", you could specify
# tasks.sieve.las.execpath=${HOME}/build/cado-nfs/debug


# verbose: Verbosity of program output. Many programs accept a -v (or
# similar) command line flag; this parameter specified whether verbose
# output should be enabled. Has no effect on cadofactor itself, just on the
# sub-programs.

tasks.verbose = 1
# An example of overriding the tasks.verbose=1 for the polyselect task
# tasks.polyselect.verbose = 0


# rlim, lpbr, mfbr, rlambda, alim, lpba, mfba, alambda: The sieving
# parameters. Also used by several other Tasks and Programs, so best
# specified at path "tasks" where everyone can find them.

tasks.rlim=50000
tasks.alim=100000
tasks.lpbr=22
tasks.lpba=22
tasks.mfbr=22
tasks.mfba=22
tasks.rlambda=1.2
tasks.alambda=1.2


# threads: number of threads to use in sub-programs.
# For programs that support multi-threading (e.g., polynomial selection,
# sieving, linear algebra), this specifies the number of threads to use.

tasks.threads=2


# For tasks that use the client/server set-up, set a time-out in seconds
# after which assigned workunits for which no result was returned get 
# cancelled and re-submitted to the database, so that other clients can 
# (hopefully) finish them

# Workunits for this test factorization are quite short, 120s is ok
tasks.wutimeout = 120

# After a total of maxtimedout workunits have timed out, the factorization
# is aborted, as such a large number of timed out workunits may indicate a
# problem with the clients. The default is 100. For large factorization on
# unreliable machines which can be stopped/restarted frequently, this limit
# is probably too small and should be increased.

tasks.maxtimedout = 100


# P, degree, admin, admax, adrange: Parameters for the polynomial search
tasks.polyselect.P = 420
tasks.polyselect.degree = 4
tasks.polyselect.admin = 0
tasks.polyselect.admax = 10000
tasks.polyselect.adrange = 5000
tasks.polyselect.maxnorm = 21.5

# define tasks.sieve.ratq to True if you want special-q's to be taken on
# the rational side (by default they are on the algebraic side), this can be
# better in the case of SNFS polynomials
tasks.sieve.ratq=False

tasks.sieve.rels_wanted = 122540
tasks.sieve.qrange = 2000
tasks.sieve.I = 11
tasks.filter.nslices_log=1
tasks.filter.keep = 160
tasks.filter.skip = 32
tasks.filter.forbw = 3
tasks.filter.coverNmax  = 100
tasks.filter.maxlevel = 15
tasks.filter.ratio = 1.1
tasks.linalg.mn = 64
tasks.linalg.verbose = 0
tasks.linalg.nchar = 64
# tasks.linalg.bwc.threads can be of the form 2x3 or in the form 6, in the
# latter case the splitting is chosen automatically
tasks.linalg.bwc.threads = 2x2

# address: The server of the client/server set-up will listen at this
# address. Note that listening on address "localhost" will allow only
# clients running on the same machine to connect. If not specified, the
# computer's hostname will be used, which usually resolves to a network-
# visible address so clients on other machines can connect.

# server.address = localhost


# port: The server of the client/server set-up will listen at this
# port. It can be useful to specify a different value (8001, 8002, ...)
# to be able to run several factorizations on the same machine.

server.port = 8012


# ssl: whether to use SSL for client/server communication.
# Without SSL authentication and encryption, it is possible for an attacker
# on the same network to modify workunits sent to clients and so effect
# execution of malicious code on the client machines. With SSL, clients
# are started with the server's certificate fingerprint as a command line
# argument, and verify the certificate when first connecting. The
# certificate is then used for all subsequent communication.
# The default is yes.

# server.ssl = yes


# whitelist: a comma separated list of IP ranges that should be allowed to
# contact the server. The IP ranges are given in CIDR format: an IP address,
# optionally followed by a slash and the number of most significant bits
# that need to match. For example, 192.168.0.0/24 means that all IP
# addresses in the range 192.168.0.0, ..., 192.168.0.255 are allowed.
# Here, hostnames can also be used in place of the IP address, their name
# will be resolved to an IP address.
# Note that the hostnames on which cadofactor starts clients, i.e., the
# hostnames in the slaves[...].hostnames parameters, are always added to the
# whitelist. You should have to whitelist addresses manually only if you
# want to start clients manually.

# server.whitelist = 192.168.0.0/24,localhost


# Clients which cadofactor should start are specified under "slaves".
# We look for any path under "slaves" with a "hostnames" key, then for each 
# hostname specified (comma separated list, with multiplicity), one slave is
# launched on that host.
# If the hostname is of the form "@filename", then the host names are read
# from the file "filename", one host name per line, with the same
# multiplicity rule.
# Note: if the hostname for a client is "localhost", then the client is
# started directly as sub-processes of the cadofactor script; if it is not
# "localhost", then clients are started through SSH. This affects the
# client's current working directory: if the client is started as a
# sub-process, its CWD is the same as that of the cadofactor script; if it
# is started through SSH, its CWD is the CWD after an SSH login (usually the
# user's home directory).

slaves.hostnames = localhost
# Example with multiple hostnames. This launches one client on localhost,
# two on otherhost1, and one on otherhost2, assuming there is no nrclients 
# parameter (see below) that would override the multiplicity
# slaves.hostnames = localhost, otherhost1, otherhost1, otherhost2


# Modifies the behaviour of the hostnames list: if nrclients is specified,
# then multiplicity in the hostnames list is ignored, and "nrclients" client
# scripts are launched on each unique host name.

slaves.nrclients = 2


# scriptpath: The path to the wuclient2.py scripts, on the slave's file
# system.

slaves.scriptpath = $(sourcedir)/scripts/cadofactor


# downloadretry: Number of seconds a client should wait between failed
# download attempts from the server

slaves.downloadretry = 10


# basepath: defines the base directory under which wuclient creates its
# download and working directories. It is permissible to let all clients of
# the same factorization to use the same download directory; files are
# protected by locking to prevent download races. If the download directory
# is on a shared file system, then this filesystem must support locking.
# The working directories should probably be unique; the default choice for
# the working directory includes the clientid which should ensure uniqueness.

slaves.basepath = /tmp/work/$(name).wuclient


# If you want to launch clients on multiple groups of slave machines, with
# different parameters per group, you can group them by different parameter
# paths, like so:

# slaves.downloadretry = 10
# slaves.basepath = /tmp/wuclient
# slaves.home.hostnames = localhost
# slaves.home.nrclients = 1
# slaves.home.scriptpath = $(sourcedir)/scripts/cadofactor
# slaves.work.desktop.hostname = workstation1, workstation2
# slaves.work.desktop.scriptpath = /path/to/scripts/cadofactor
# slaves.work.cluster.hostname = clusternode1, clusternode2, clusternode3
# slaves.work.desktop.scriptpath = /other/path/to/scripts/cadofactor
## Use 4 slaves on all "work" slaves, both "desktop" and "cluster"
# slaves.work.nrclients = 4

# Here, "home", "work", "workstation", and "cluster" are arbitrary 
# alphanumeric strings; their sole purpose is to create different nodes in 
# the parameter tree. 

# A slightly advanced feature is prefixing the command line that is used on
# the client machine to run wuclient2.py by a word of your choosing. This
# could be a shell wrapper that sets up the execution environment, or a
# particular choice of Python interpreter, etc.
# For the slave machines specified in "slaves.hostnames", the "runprefix"
# parameter is searched in "slaves.wuclient.runprefix" (and therefore, if
# not present there, in "slaves.runprefix", then in "runprefix"). For 
# slaves specified in slaves.work.desktop.hostname, the search starts in
# "slaves.work.desktop.wuclient.runprefix".

# This example uses the debugging-enabled Python 3.2 interpreter to run
# wuclient2.py.
# slaves.wuclient.runprefix = /usr/bin/python3.2-dbg
